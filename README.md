# Example of Spark Scala ETL with tests

1. Ссылка на репозиторий
https://github.com/bushmanov/rostel

2. В корне лежит console_output с логом исполнения

3. В папке Jupyter ноутбук с интерактивной сессией

4. Скомпилированный Jar файл приложения в targets

5. Данные "обогащены" файлом с плохими данными: "bad_logs.tsv"

6. Данные читаются из Hadoop, директория с числом в имени, справочники тоже читаются из Hadoop.

7. Запрошенные таблицы пишутся в Hive как df.saveAsTable(). В 1.6 работало не очень корректно, сейчас вроде бы проблем нет ни с записью, ни с чтением, как из Spark, так и из Hive. Почти нет.

8. Spark сессия broadcast стоит по умолчанию, сделал в явном виде (с планом), чтобы показать, что я понимаю, о чем идет речь. То же самое касается некоторых других настроек. 

9. Код исполнялся на локальной машине, в кластере может сразу не взлететь, я полагаю это вопрос внесения минимальных изменений в код. У меня код отработал (см. логи в корне).

10. Тесты. Тестирую 3 функции на выбор из основного класса на тестовых данных из отдельной директории (на данный момент -- копия 2019-12-12, но лежит отдельно). Сравниваю таблицы с бенчмарком, который сохранен в отдельной БД Hive: rostel_test.

11. Метрики -- количество пропусков (null), данных в некорректных форматах, и не имеющих значений в справочниках (включая некорректные url). URL джойнил по хостам, но теоретически можно взять глубину больше (например +1/2 папки вглубь от корня). Вопрос обсуждения. Помимо количеств некорректных данных, все некорретные данные выводятся в отдельную таблицу для последующего анализа, что пошло не так. Первая итерация чтения (кэшированная, т.к. к этой таблице потом обращаются все остальные таблицы) -- это стринги (без типирования), поэтому в конце фильтрую все данные по айдишникам, и можно посмотреть что было не так в сырых данных. И я добавил айдишники!

12. Кроме метрик плохих данных есть тайминг исполнения каждого шага пайплайна (выводится в консоль).

13. Big-O:

        - логи clickstream обогащенные mrf из справочника регионов: N*log(N) *1/P, где N -- размер большей таблицы, P -- партиций. Сложность (асимптотическая) джойна, т.е. сортировки с поправкой на количество партиций.
        - пользователь с полем first_session. Сначала отсортировали по пользователю, потом еще раз по времени. Асимптотически это тоже N * log(N) * 1/P, можно умножить на 2.
        - логин, дата, popular_category - самая популярная по количеству посещений категория за день. Группировка (N*log(N)*1/P) по пользователям, затем второй раз по категориям примерно с той же самой сложностью, затем   сортировка, в самомом плохом случае у каждого пользователя все катеогрии разные n*log(n), где n - количество категорий. Итого 2*N*log(N) + n*log(n). 
